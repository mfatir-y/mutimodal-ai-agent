# AI Agent with Ollama Integration

A powerful RAG-based (Retrieval-Augmented Generation) code generation platform that leverages local Large Language Models (LLMs) to turn natural language prompts into functional code, with built-in document understanding, feedback collection, and model evaluation systems.


## üöÄ Features

- **Multiple LLM Support**: Integrates with different Ollama models
- **Document Analysis**: Processes and analyzes PDF documents using LlamaParse
- **Code Reading & Analysis**: Specialized tools for code comprehension
- **Interactive Interface**: Streamlit-based web interface for easy interaction
- **Vector Search**: Efficient document indexing and retrieval using vector store
- **Feedback Analysis**: Built-in feedback management and AI analysis system
- **RAG System for Generation**: Context-aware code generation that references uploaded materials

## üìã Requirements

- Python 3.8+
- Ollama installed and running locally
- LlamaCloud API key for PDF parsing

### Key Dependencies

```txt
ollama
streamlit
llama-index
transformers
pandas
matplotlib
pydantic
```

For a complete list of dependencies, see `requirements.txt`.

## üõ†Ô∏è Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd ai_agent
```

2. Install Ollama:
- For Windows WSL2/Linux:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```
- For MacOS:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```
- For Windows (native):
Download and install from [Ollama Releases](https://github.com/ollama/ollama/releases)

3. Pull required (or desired) Ollama models:
```bash
# Pull the chat model
ollama run mistral

# Pull the code analysis model
ollama run codellama
```

4. Create and activate a virtual environment (recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

5. Install dependencies:
```bash
pip install -r requirements.txt
```

6. Set up environment variables:
Create a `.env` file in the project root and add:
```
LLAMACLOUD_API_KEY=your_api_key_here
```

## üö¶ Usage

1. Start the Streamlit application:
```bash
streamlit run streamlitApp.py
```

2. The application will be available at `http://localhost:8501`
3. Generating Code
- Select your preferred models in the sidebar
- Upload any reference files the AI should consider
- Enter your prompt in the text area (e.g., "Create a function to read CSV files and calculate the average of each column")
- Click "Generate Response"
- Provide feedback on the generated code

## üèóÔ∏è Project Structure

- `main.py`: Core initialization and LLM setup
- `streamlitApp.py`: Web interface implementation
- `feedback_manager.py`: Feedback collection and management
- `feedback_analyzer.py`: Analysis of collected feedback
- `model_evaluator.py`: Model performance evaluation
- `code_reader.py`: Code analysis utilities
- `prompts.py`: System prompts and templates
- `model_registry.py`: Model registration and management

## üîß Configuration

The system supports multiple LLM models through Ollama:
- Default chat model: Mistral
- Code analysis model: CodeLlama

Models can be configured in the initialization parameters or through the web interface.

## üìä Features in Detail

### Document Analysis
- Processes PDF documents with LlamaParse for structured extraction
- Utilizes BGE-M3 embeddings for efficient document indexing
- Supports semantic search with customizable relevance thresholds

### Code Analysis
- Real-time code parsing and analysis using CodeLlama
- Contextual code understanding with documentation integration
- Support for multiple programming languages

### Performance Metrics
- Response Time: Average response time <30 seconds for queries and document parsing
- Memory Usage: Efficient resource management with <4GB RAM usage
- Accuracy: 
  * Code Analysis: >80% accuracy in code comprehension tasks
  * Document Search: >90% relevance in document retrieval
- Scalability: Handles documents up to 200MB in size

### Feedback System
- User feedback collection
- Performance metrics tracking
- Continuous improvement analysis

## ü§ù Contributing

Contributions are welcome! Please feel free to submit pull requests.

## üìù License

This project is licensed under the MIT License.

## üîó Additional Resources

- [Ollama Documentation](https://ollama.ai/docs)
- [LlamaIndex Documentation](https://docs.llamaindex.ai/)
- [Streamlit Documentation](https://docs.streamlit.io/) 
